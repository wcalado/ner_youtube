{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><center>How to Train spaCy NER Model</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>January 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Preparing Data for Training<br>\n",
    "2) Converting Training Data to .spacy Binary<br>\n",
    "3) How to Create a spaCy 3 config.cfg File<br>\n",
    "4) How to Train a Model in spaCy 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Training a Machine Learning Model in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook, we created a basic training set for a machine learning model using spaCy's EntityRuler. We were able to do this by making certain presumptions about things that are very likely or certainly going to fall under a specific label. Such an approach to cultivating a training set is, by its nature, problematic. It will miss some entities and falsely label others. If one wishes this to be the essential training set used to train a final model, I encourage a manual check. If, however, you want to use this model as a baseline model that can be used to cultivate a better training set via Prodigy, then this method will also work.\n",
    "\n",
    "In this notebook, we will not be interested in the refining of this training set, rather the use of it to train a custom spaCy machine learning NER model. The methods, therefore, will receive the chief focus on this notebook, not the results.\n",
    "\n",
    "In 01.04: Machine Learning NER, we first met machine learning and some of the fundamentals of it. If you have not viewed that notebook and the videos within, I encourage you to do so prior to working through this notebook as I will be assuming that you have a basic understanding of machine learning.\n",
    "\n",
    "The reason I prefer spaCy over other NLP frameworks is spaCy's ability to scale well (work on both small and big data) and it's easy-to-use training process. An NER practitioner does not have to create a custom neural network via PyTorch/FastAI or TensorFlow/Keras, all of which have a steep learning curve, despite being some of the easiest frameworks to use. Instead, users of spaCy can take advantage of the predesigned CNN architecture behind the spaCy training process. In version 3.0 of spaCy (nightly is available at the time of writing this notebook), due in early 2021, the user will also be able to customize this neural network architecture, expanding spaCy's utility and customizability.\n",
    "\n",
    "In order to take advantage of the spaCy training process, the user need only understand a few basic concepts, such as how the data should look going into the training process (covered in the last notebook) and a few hyperperameters (the things we adjust in the training process to try and find optimal results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the last notebook, your input data should be in the following format:\n",
    "\n",
    "TRAIN_DATA = [\n",
    "              (TEXT AS A STRING, {\"entities\": [(START, END, LABEL)]})\n",
    "              ]\n",
    "\n",
    "To begin, let's bring in the code from the last video to generate our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Treblinka is a small village in Poland.', {'entities': [[0, 9, 'GPE']]}], ['Wikipedia notes that Treblinka is not large.', {'entities': [[21, 30, 'GPE']]}]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Treblinka is a small village in Poland. Wikipedia notes that Treblinka is not large.\"\n",
    "corpus = []\n",
    "\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    corpus.append(sent.text)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "TRAIN_DATA = []\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "    entities = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        entities.append([ent.start_char, ent.end_char, ent.label_])\n",
    "    TRAIN_DATA.append([sentence, {\"entities\": entities}])\n",
    "\n",
    "print (TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Convert the Training Data to spaCy Binary Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous version of this textbook, we used spaCy 2. The way in which we train in spaCy 3 is entirely different. While it is possible to work through some of the fundamentals in a script, I think it is best done in the terminal. Because this textbook is a JupyterBook, which sits on top of many Jupyter Notebooks, we can execute terminal-based commands with an ! at the start of a cell. In order to train a machine learning model, the first thing that we need to do is to create a spaCy binary object of that training data. I find it is always good to use a function if a bit of code is going to repeated in the future, that way it can be reproducible. The following function is available on spaCy's repo: https://github.com/explosion/projects/blob/v3/pipelines/ner_demo/scripts/convert. I modified it slightly to work better in this textbook.\n",
    "\n",
    "Let's break down what this function is doing. It's entire purpose is to convert our spaCy 2 formatted TRAIN_DATA into spaCy 3 binary training data. In my workflow, I like to keep these two steps separate. The reason is that I like to be able to examine and verify my training data prior to converting it to spaCy 3 binary format. It allows for one final quick manual validation. The function takes three arguments:\n",
    "\n",
    "1) lang => this will be the language of the blank model. Use \"en\" for English, \"de\" for German, etc.<br>\n",
    "2) TRAIN_DATA => this will be the training data as a list of lists like we saw above.<br>\n",
    "3) output_path => this will be the output directory in which the spaCy binary file will sit.<br>\n",
    "\n",
    "The nice thing about this function is if your training data does not align, it will simply be ignored. This will prevent errors in the training process. In order for this function to work, it needs to create a DocBin object to save. The **db = DocBin()** allows us to run **db.add()** and add in our training data individually. These are then converted to binary objects (smaller in size and faster to load). and saved to disk with **db.to_disk()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "import typer\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def convert(lang: str, TRAIN_DATA, output_path: Path):\n",
    "    nlp = spacy.blank(lang)\n",
    "    db = DocBin()\n",
    "    for text, annot in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n",
    "                warnings.warn(msg)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TKoPva69_6E\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TKoPva69_6E\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the function is ready, we can call. In this simple, toy example, we are going to do (intentionally) a major mistake in ML. We are going to use not only a small training data sample (only 2), we are going to use both for training and validation. I cannot stress this enough. NEVER DO THIS. We are doing it here, simply to establish a workflow and demonstrate some of the problems that this method of small training data can present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(\"en\", TRAIN_DATA, \"data/train.spacy\")\n",
    "convert(\"en\", TRAIN_DATA, \"data/valid.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the spaCy config.cfg File and How do I create it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data ready, it's time to start preparing our model. In spaCy 3, we have a lot of control over the neural network architecture and hyperperameters of our model. This all takes place in the new config.cfg file. This config file is giving to spaCy during the training process so that it knows what to train and how. In order to create the config.cfg file, we first need to create a base_config.cfg file. To do that, we can use spaCy's handy GUI, found here: https://spacy.io/usage/training (scroll down a bit). You will find something that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spaCy_config_gui.JPG\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, select, \"English\", the language that we are training, \"ner\" only, the model we are training, \"CPU\" (GPU is a bit more complex), and efficiency (quicker to train and smaller because there are no word vectors). You will copy and paste the output in the GUI into your directory as \"base_config.cfg\". We will only make two minor changes to this base_config.cfg file. We will specify the path of train and dev (seen under the first category of paths). We will set these to the location of our train.spacy and valid.spacy files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/paths.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the base_config file is setup correctly, it's time to convert it to a config.cfg file. To do that, we need to execute a terminal command. Fortunately, we can do that here in Jupyter Notebook. I have placed my base_config file in the subfolder data. By running the command below, spaCy reformats the base_config into a properly formatted config.cfg file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "data\\config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:02:33.694893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config data/base_config.cfg data/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l67PXnhu0ig\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l67PXnhu0ig\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the config.cfg file in place, we can train our first model. In our case, I will be placing our models in the subfolder models/output. We run the command below, and we have a trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Train a spaCy 3 Model from the config.cfg File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00      7.83   25.00   14.29  100.00    0.25\n",
      "200     200          0.20     98.13  100.00  100.00  100.00    1.00\n",
      "400     400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "600     600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "800     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1000    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1200    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1400    1400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1600    1600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1800    1800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "[+] Saved pipeline to output directory\n",
      "models\\output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 08:02:39.418504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n",
      "[2021-08-10 08:02:42,192] [INFO] Set up nlp object from config\n",
      "[2021-08-10 08:02:42,198] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-08-10 08:02:42,200] [INFO] Created vocabulary\n",
      "[2021-08-10 08:02:42,201] [INFO] Finished initializing nlp object\n",
      "[2021-08-10 08:02:42,254] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train data/config.cfg --output ./models/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output tells us the epochs, the number of samples, as well as some metrics for our model. Our model shows 100%, but this does not mean we have a good model. It is overfitted, meaning it essentially memorized one sample, Treblinka. Nevertheless, let's load up the model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "trained_nlp = spacy.load(\"models/output/model-best\")\n",
    "text = \"The village of Treblinka is located in Poland.\"\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we gave the machine learning model NER a new sentence and it correctly identifies Treblinka as a \"GPE\". But we should not get too excited. Minor alterations to this text result in a missed entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No entities found.\n"
     ]
    }
   ],
   "source": [
    "text = \"Mark, from New York, said that he wants to go to Treblinkaa to speak to the locals.\"\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)\n",
    "if len(doc.ents) == 0:\n",
    "    print (\"No entities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does our model now fail? Because we have trained a machine learning model, not an EntityRuler. It knows that Treblinka is a GPE, but it has only learned to identify it if it is spelled correctly. This is a bad model. Machine learing NER models improve with the more training data that we feed them. Most importantly, however, they improve with the greater amount of varied training data we feed them. A good rule of thumb is to start with 200 training samples and then make adjustments going forward. You may need to gather more varied training data or you may need to reconsider your labels. Another possibility is that you need to fine-tune your hyperperameters in the config.cfg file. We will be covering these problems and solutions throughout the remainder of this textbook. By now, though, you should have a good sense of how the training process works in spaCy 3. The material discussed in this notebook are by far the most challenging so far. Take your time here and get to know this process well before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excerise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook's exercise, try and generate a larger quantity of training data and feed it into a spaCy model and test that model on new, unseen texts. See how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I rushed through this process with little exposition. The reason is because this material is easier to cover in video form. Please see the video below in which we perform similar tasks on a larger scale with the characters from the first book of Harry Potter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PJZzBp6em-Q\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PJZzBp6em-Q\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
